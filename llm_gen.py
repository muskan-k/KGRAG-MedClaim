# -*- coding: utf-8 -*-
"""LLM-GEN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwPrrf9MePnTIHry99ErHaFGEvWx6clV
"""

# script to download mistral LLM

import os
import json
from huggingface_hub import hf_hub_download
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.colab import userdata

MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2"
LOCAL_DIR = "models/Mistral-7B-Instruct-v0.2"
os.makedirs(LOCAL_DIR, exist_ok=True)

TOKEN = userdata.get('HF_TOKEN')

# Function to download shards
def download_shard(shard_name):
    print(f"Downloading shard {shard_name}…")
    hf_hub_download(
        repo_id=MODEL_ID,
        filename=shard_name,
        local_dir=LOCAL_DIR,
        use_auth_token=TOKEN,
        local_files_only=False
    )
    return shard_name

# Function to download model
def download_model():
    # Download index files
    for fname in [
        "config.json",
        "generation_config.json",
        "model.safetensors.index.json",
        "pytorch_model.bin.index.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
    ]:
        print(f"Downloading {fname}…")
        hf_hub_download(
            repo_id=MODEL_ID,
            filename=fname,
            local_dir=LOCAL_DIR,
            use_auth_token=TOKEN,
            local_files_only=False
        )

    # Read safetensors index and schedule each shard
    with open(os.path.join(LOCAL_DIR, "model.safetensors.index.json")) as f:
        st_index = json.load(f)
    safetensor_shards = list(set(st_index["weight_map"].values()))

    # Read bin index and schedule each shard (optional; skip if you only want safetensors)
    with open(os.path.join(LOCAL_DIR, "pytorch_model.bin.index.json")) as f:
        bin_index = json.load(f)
    bin_shards = list(set(bin_index["weight_map"].values()))

    # Combine them or pick one format:
    # To download only safetensors, use safetensor_shards.
    # To download only bin, use bin_shards.
    shards = safetensor_shards  # or bin_shards

    with ThreadPoolExecutor(max_workers=6) as exe:
        futures = [exe.submit(download_shard, s) for s in shards]
        for fut in as_completed(futures):
            print("Finished:", fut.result())
    print("All shards downloaded!")

download_model()

# Training!

import argparse
import os
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model

base_model="models/Mistral-7B-Instruct-v0.2"

OUTPUT_DIR = "healthver-sft"
SEQ_LEN    = 400
#clone & load CSVs
if not os.path.isdir("HealthVer"):
    os.system("git clone https://github.com/sarrouti/HealthVer.git")
train_df = pd.read_csv("HealthVer/data/healthver_train.csv")
val_df   = pd.read_csv("HealthVer/data/healthver_dev.csv")
train_ds = Dataset.from_pandas(train_df)
val_ds   = Dataset.from_pandas(val_df)

tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        use_fast=True,
        offload_state_dict=True
    )
tokenizer.model_max_length = SEQ_LEN
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def preprocess(ex):
    prompt = (
        "Evidence:\n" + ex["evidence"]
        + f"\n\nClaim: {ex['claim']}\nQuestion: {ex['question']}"
    )
    response = ex["label"]
    text = prompt + "\n### Response: " + response
    return tokenizer(text, truncation=True, max_length=SEQ_LEN)

train_tok = train_ds.map(preprocess, remove_columns=train_ds.column_names)
val_tok   = val_ds.map(preprocess,   remove_columns=val_ds.column_names)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    offload_state_dict=True
)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.half().to(device)

for p in model.parameters():
        p.requires_grad = False

lora_cfg = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
model = get_peft_model(model, lora_cfg)
model.config.use_cache = False

model.config.label_names = ["labels"]

for n, p in model.named_parameters():
        if "lora" in n:
            assert p.requires_grad
        else:
            assert not p.requires_grad

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        do_eval=True,
        eval_steps=500,
        save_steps=500,
        logging_steps=100,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=3,
        fp16=torch.cuda.is_available(),
        learning_rate=2e-5,
        warmup_steps=100,
        save_total_limit=2,
        report_to=[],  # disable wandb
    )
trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=val_tok,
        data_collator=data_collator,
    )

trainer.train()

from google.colab import drive
import os

# 1) Mount Drive (you'll get a link to authorize)
drive.mount('/content/drive')

# 2) Choose an output path in your Drive
OUTPUT_DIR = '/content/drive/MyDrive/healthver_sft'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 3) After training finishes:
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"Saved LoRA adapters and tokenizer to {OUTPUT_DIR}")

# KG Creation

!pip install neo4j

from transformers import pipeline
import pandas as pd, json
from neo4j import GraphDatabase

extractor = pipeline(
  "text-generation",
  model=model,
  tokenizer=tokenizer,
  device=0,
  return_full_text=False
)
IE_PROMPT = """You are an information extraction model for biomedical research abstracts.
Extract complete triplets of the form:
  - object: biomedical noun phrase
  - subject: biomedical noun phrase
  - predicate: short relation verb or verb phrase

Respond with ONLY valid JSON like this:

{
  "triplets": [
    {"object": "headache", "subject": "aspirin", "predicate": "treats"},
    {"object": "COVID-19",   "subject": "vaccine", "predicate": "prevents"}
  ]
}

Rules:
•⁠  ⁠Each triplet MUST contain all 3 fields: object, subject, predicate.
•⁠  ⁠Do not return any nulls or missing fields.
•⁠  ⁠Use clear biomedical nouns or noun phrases for object.
•⁠  ⁠Do not include explanation, markdown, or natural language text.
"""

import re
df = pd.read_csv("https://raw.githubusercontent.com/sarrouti/HealthVer/master/data/healthver_dev.csv")

for i, row in df.iterrows():
    inp = IE_PROMPT + "\n\nText: " + row['evidence']
    raw = extractor(inp, max_new_tokens=1000, do_sample=False)[0]['generated_text']
    print(raw)
    try:
      match = re.search(r'\{.*"triplets"\s*:\s*\[.*?\]\s*\}', raw, re.DOTALL)
      if not match:
          raise ValueError("No valid triplets JSON block found.")
      data = json.loads(match.group(0)) # {"triplets":[{...},...]}
    except Exception as e:
      print(e)
      data = {"triplets":[]}
    df.at[i, 'triplets'] = data['triplets']

# ——————————————————————————
uri      = 'bolt://3.86.101.25'
username = 'neo4j'
password = 'solders-alibi-prepositions'
driver   = GraphDatabase.driver(uri, auth=(username, password))

# Write both subgraphs in one batch
with driver.session(database="neo4j") as session:
    # Merge Claim/Evidence nodes + labeled edges
    session.run(
        """
        UNWIND $dev AS row
          MERGE (c:Claim    {id: row.claim_id})    SET c.text = row.claim
          MERGE (e:Evidence {id: row.evidence_id}) SET e.text = row.evidence
          FOREACH (_ IN CASE WHEN row.label='SUPPORTS'        THEN [1] ELSE [] END |
            MERGE (c)-[:SUPPORTS]->(e))
          FOREACH (_ IN CASE WHEN row.label='REFUTES'         THEN [1] ELSE [] END |
            MERGE (c)-[:REFUTES]->(e))
          FOREACH (_ IN CASE WHEN row.label='NOT_ENOUGH_INFO' THEN [1] ELSE [] END |
            MERGE (c)-[:NOT_ENOUGH_INFO]->(e))
        """,
        dev=df.to_dict('records')
    )

    # 4b) Merge Entity/Entity nodes + dynamic triplet edges
    for t in df['triplets']:
        subj = t["subject"]
        obj  = t["object"]
        pred = (t.get("predicate") or "").strip()
        if not pred:
            continue
        # sanitize label
        lbl = re.sub(r'\W+', '_', pred).strip('_').upper()
        lbl = lbl or "RELATION"
        # execute one MERGE per triplet
        session.run(
            f"""
            MERGE (s:Entity {{name:$subj}})
            MERGE (o:Entity {{name:$obj}})
            MERGE (s)-[:`{lbl}`]->(o)
            """,
            subj=subj, obj=obj
        )

driver.close()

# import json
# from typing import List, Tuple

# with open("triplets.json", "r") as f:
#     _KG_DUMP = json.load(f)

# # Sanity check
# print("Top‐level keys:", _KG_DUMP.keys())

# # Build a flat list of (subject, predicate, object) tuples
# _all_triplets: List[Tuple[str,str,str]] = [
#     (t["subject"], t["predicate"], t["object"])
#     for t in _KG_DUMP.get("triplets", [])
# ]

# print(f"🔍 Loaded {_all_triplets!r} (showing first 5):\n", _all_triplets[:5])

# def get_triples_for_claim(claim: str, limit: int = 20) -> List[Tuple[str,str,str]]:
#     """
#     Since this JSON is for one claim only, we ignore `claim` and just
#     return up to `limit` of the precomputed triples.
#     """
#     return _all_triplets[:limit]

#### KG Retrieval

!pip install neo4j

from neo4j import GraphDatabase
import os
from typing import List, Tuple

# Connection settings
os.environ['NEO4J_URI']      = 'bolt://3.86.101.25'
os.environ['NEO4J_USERNAME'] = 'neo4j'
os.environ['NEO4J_PASSWORD'] = 'solders-alibi-prepositions'
os.environ['NEO4J_DATABASE'] = 'neo4j'

driver = GraphDatabase.driver(
    os.environ['NEO4J_URI'],
    auth=(os.environ['NEO4J_USERNAME'], os.environ['NEO4J_PASSWORD'])
)

# --- Placeholder entity extractor (replace with NLP method like scispacy, MetaMap, etc.)
def _extract_entities(text: str) -> List[str]:
    # Simplified: split by space and remove punctuation
    import re
    tokens = re.findall(r'\b\w+\b', text)
    return [t for t in tokens if len(t) > 3]  # Filter short/common words

# --- Triplet retriever based on fuzzy match with entities from claim
def get_triples_by_entity_similarity(claim: str, limit: int = 20) -> List[Tuple[str, str, str]]:
    ents = _extract_entities(claim)
    if not ents:
        return []

    ents_low = [e.lower() for e in ents]

    cypher = """
    UNWIND $ents AS term
    MATCH (s)-[r]->(o)
    WHERE toLower(s.name) CONTAINS term
       OR term CONTAINS toLower(s.name)
       OR toLower(o.name) CONTAINS term
       OR term CONTAINS toLower(o.name)
    RETURN DISTINCT s.name AS subject,
                    type(r) AS predicate,
                    o.name AS object
    LIMIT $limit
    """

    with driver.session(database=os.environ['NEO4J_DATABASE']) as session:
        res = session.run(cypher, {"ents": ents_low, "limit": limit})
        return [(row["subject"], row["predicate"], row["object"]) for row in res]

#### PUBMED RETRIEVAL

!pip install rank_bm25
!pip install scispacy

!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz

!pip install -U spacy

import os
import re
import requests
import xml.etree.ElementTree as ET
from functools import lru_cache
from typing import List, Dict
from rank_bm25 import BM25Okapi
import spacy

# Configure your email and (optionally) API key in your Colab environment:
#   %env ENTREZ_EMAIL=your_email@example.com
ENTREZ_EMAIL = os.getenv("ENTREZ_EMAIL", "amintarebecca.ar@gmail.com")
ENTREZ_KEY   = os.getenv("ENTREZ_KEY",   "eabfe9be67edd16a3f6f3bfedd0924ce2509")
NCBI_BASE    = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"

# load the SciSpaCy model once
_nlp = spacy.load("en_core_sci_lg")

_EN_STOP = {
    "a","an","the","of","to","and","in","on","for","with","into","from","by",
    "about","is","are","was","were","be","been","being","as","that","this",
    "these","those","it","its","their","at","or","not","but","than","then",
}

def _tokenise(text: str) -> List[str]:
    """Lower-case, strip punctuation, remove trivial stop-words."""
    words = re.findall(r"[A-Za-z0-9-]+", text.lower())
    return [w for w in words if w not in _EN_STOP and len(w) > 2]

def _extract_bio_entities(text: str) -> set:
    """Use SciSpaCy to pull out biomedical named entities (lemmas)."""
    doc = _nlp(text)
    return {ent.lemma_.lower() for ent in doc.ents}

@lru_cache(maxsize=1024)
def _mesh_synonyms(term: str) -> List[str]:
    """
    Use PubMed’s automatic term‐mapping to find MeSH synonyms for a single token.
    """
    try:
        resp = requests.get(
            f"{NCBI_BASE}/esearch.fcgi",
            params={"db":"pubmed","term":term,"retmode":"json","retmax":0},
            timeout=6
        )
        resp.raise_for_status()
        stack = resp.json()["esearchresult"].get("translationstack", [])
        mesh_terms = set()
        def _collect(e):
            if isinstance(e, dict) and e.get("Field","").lower()=="mesh terms":
                txt = e["term"]
            elif isinstance(e, str) and e.endswith("[MeSH Terms]"):
                txt = e
            else:
                return
            cleaned = re.sub(r'\[.*\]$',"", txt).strip('"').lower()
            if cleaned and cleaned != term.lower():
                mesh_terms.add(cleaned)
        for elem in stack:
            if isinstance(elem, list):
                for sub in elem:
                    _collect(sub)
            else:
                _collect(elem)
        return sorted(mesh_terms)
    except Exception:
        return []

def _build_pubmed_query(claim: str, verbose: bool = True) -> str:
    """
    Tokenise the claim → extract biomedical entities → only MeSH‐expand
    those entities → fallback on entity tokens via [TIAB] → if no entities,
    fallback on all tokens via [TIAB].
    """
    toks     = _tokenise(claim)
    entities = _extract_bio_entities(claim)

    mesh_chunks = []
    fallback    = []

    if verbose:
        print("\n[Bio‐entities found]")
        print(" ", entities)

    # 1) only consider entity tokens for MeSH
    if verbose:
        print("toks:", toks)
    for t in toks:
        if t not in entities:
            continue

        syns = _mesh_synonyms(t)
        if syns:
            if verbose:
                print(f"  {t:>12} → {', '.join(syns)}")
            mesh_chunks += [f'"{s}"[MeSH Terms]' for s in syns]
        else:
            # no MeSH found, but t is a bio‐entity → fallback to TIAB
            fallback.append(f"{t}[TIAB]")

    all_terms = mesh_chunks + fallback
    if verbose:
        print("all_terms:", all_terms)
        print("entities:", entities)

    # for entity in entities:
    #     term = f"{entity}[TIAB]"
    #     if entity not in all_terms and term not in all_terms:
    #         fallback.append(term)
    # 2) if there were *no* entity tokens at all, fallback on all toks
    if not entities:
        if verbose:
            print("No bio‐entities found; falling back to all tokens as [TIAB].")
        fallback = [f"{w}[TIAB]" for w in toks]
    if verbose:
      print("\n→ Final MeSH chunks:\n  ", " OR ".join(mesh_chunks), "\n")
    query = " OR ".join(mesh_chunks + fallback)

    if verbose:
        print("\n→ Final PubMed query:\n  ", query, "\n")
    return query

def _search_pubmed(term: str, retmax: int = 100) -> List[str]:
    """eSearch → list of PMIDs."""
    params = {
        "db":"pubmed","term":term,
        "retmode":"json","retmax":retmax,
        **({"api_key":ENTREZ_KEY} if ENTREZ_KEY else {}),
        "email":ENTREZ_EMAIL,
    }
    r = requests.get(f"{NCBI_BASE}/esearch.fcgi", params=params, timeout=10)
    r.raise_for_status()
    return r.json()["esearchresult"].get("idlist", [])

def _fetch_abstracts(pmids: List[str]) -> Dict[str,str]:
    """Fetch title + abstract for every PMID in one go, robust to missing AbstractText."""
    if not pmids:
        return {}
    params = {
        "db":"pubmed",
        "id":",".join(pmids),
        "retmode":"xml",
        "rettype":"abstract",
    }
    r = requests.get(f"{NCBI_BASE}/efetch.fcgi", params=params, timeout=20)
    r.raise_for_status()
    root = ET.fromstring(r.content)
    out = {}
    for art in root.findall(".//PubmedArticle"):
        pmid_el = art.find(".//PMID")
        pmid = pmid_el.text if pmid_el is not None else None
        if not pmid:
            continue
        title_el = art.find(".//ArticleTitle")
        title = title_el.text or ""
        abs_nodes = art.findall(".//Abstract/AbstractText")
        if abs_nodes:
            abst = " ".join([t.text.strip() for t in abs_nodes if t.text])
        else:
            abst = ""
        out[pmid] = f"{title}. {abst}"
    return out

def get_relevant_abstracts(claim: str, top_k: int = 5) -> List[str]:
    """
    Full pipeline: entity-driven MeSH expansion → PubMed IDs → fetch →
    BM25 re-rank → return top_k.
    """
    query = _build_pubmed_query(claim, True)
    pmids = _search_pubmed(query, retmax=100)
    if not pmids:
        print("No PMIDs found for that query.")
        return []
    abs_map = _fetch_abstracts(pmids)
    if not abs_map:
        print("Couldn’t fetch abstracts for PMIDs:", pmids[:5])
        return []
    corpus = list(abs_map.values())
    bm25   = BM25Okapi([_tokenise(d) for d in corpus])
    scores = bm25.get_scores(_tokenise(claim))
    ranked = sorted(zip(abs_map.keys(), corpus, scores),
                    key=lambda x: x[2], reverse=True)
    return [f"{i+1}. {text}" for i, (_, text, _) in enumerate(ranked[:top_k])]

#### Validation

from google.colab import drive
drive.mount('/content/drive')

MODEL_DIR = '/content/drive/MyDrive/healthver_sft'

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel, PeftConfig

SEQ_LEN = 520

BASE_MODEL = 'models/Mistral-7B-Instruct-v0.2'
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16
)
base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, local_files_only=True)
base_tokenizer.model_max_length = SEQ_LEN

from transformers import pipeline
from typing import List, Tuple, Dict
import torch
import json

def verify_claim(
    model,
    tokenizer,
    claim: str,
    kg_triples: List[Tuple[str,str,str]],
    k: int = 5,
    max_triples: int = 5,
    max_abs_len: int = 300,
    use_pubmed = True,
    use_kg = False,
    use_cot = False,
    use_ift = False
) -> Dict[str,object]:
    demo_examples = [
        {
            "claim": "COVID-19 vaccines can alter a person's DNA.",
            "evidence": [
                "mRNA remains in cytoplasm and is degraded; it does not integrate into the genome."
            ],
            "verdict": "REFUTED",
            "justification": (
                "mRNA vaccines do not enter the nucleus and are broken down shortly after injection, "
                "so they cannot change human DNA."
            )
        },
        {
            "claim": "Wearing face masks reduces the spread of respiratory viruses.",
            "evidence": [
                "Clinical trials show mask use lowers transmission of droplets containing viruses."
            ],
            "verdict": "SUPPORTED",
            "justification": (
                "Masks filter respiratory droplets, which are a primary transmission route for viruses, "
                "leading to reduced infection rates."
            )
        }
    ]
    # 1) Retrieve & truncate top-k abstracts
    abstracts = get_relevant_abstracts(claim, top_k=k)
    if abstracts:
        # print("\n=== Retrieved abstracts ===")
        # for i, a in enumerate(abstracts, 1):
        #     print(f"{i}. {a}\n")
        truncated = [
            a[:max_abs_len] + ("…" if len(a) > max_abs_len else "")
            for a in abstracts
        ]
    else:
        print("No abstracts found.")
        truncated = []

    # 2) Keep only the top ⁠ max_triples ⁠ KG facts
    triples = kg_triples[:max_triples]

    # 3) Build the prompt just like in rag_pipeline.py
    prompt = f"You are a medical claim‐verification assistant."
    if use_ift:
      prompt += "\nHere are some examples:\n"
      parts = []
      for i, ex in enumerate(demo_examples, 1):
          parts.append(f"Example {i}:")
          parts.append(f"Claim: {ex['claim']}")
          parts.append("Evidence:")
          for idx, snippet in enumerate(ex['evidence'], 1):
              parts.append(f"  {idx}. {snippet}")
          parts.append(f"Verdict: {ex['verdict']}")
          parts.append(f"Justification: {ex['justification']}\n")
      prompt += "\n".join(parts)
    evidences = []
    if use_kg:
      evidences.append("one KG triple")
      prompt += f"""
Knowledge‐Graph evidence: {json.dumps([{"subj": s, "pred": r, "obj": o} for s,r,o in triples], indent=2)}
"""
    if use_pubmed:
      evidences.append("PubMed abstract")
      prompt += f"""
PubMed abstracts: {json.dumps(truncated, indent=2)}
"""
    evidence = " or ".join(evidences)
    prompt += f"""
Task:
  1. Decide whether the claim is SUPPORTED, REFUTED, or NOT ENOUGH INFO.
  2. Use {evidence} as evidence.
     *Any contradicts* → REFUTED
     *Else any supports* → SUPPORTED
     *Otherwise* → NOT_ENOUGH_INFO

Claim: {claim}
"""
    if use_cot:
      # Visible COT
  #       prompt += f"""
  # Let's think step by step:
  # 1.⁠ ⁠For each KG triple, ask: does it support or contradict the claim?
  # 2.⁠ ⁠For each abstract snippet, does it support or contradict the claim?
  # 3.⁠ ⁠If any evidence contradicts, conclude REFUTED.
  #   Else if any evidence supports, conclude SUPPORTED.
  #   Otherwise conclude NOT ENOUGH INFO.
  # """
        # Hidden COT
        prompt += (
        "Work everything out privately inside <scratchpad> … </scratchpad>.\n"
        "After </scratchpad>, do a brief SELF-CHECK: if your evidence does NOT "
        "actually justify the verdict, revise it.  Then output ONLY the JSON.\n"
        )
    prompt += "Answer in JSON format with keys ⁠ verdict ⁠ and ⁠ justification ⁠\n"

    # 4) Invoke the LLM pipeline
    gen = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1,
        return_full_text=False,
        max_new_tokens=512,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )
    raw = gen(prompt)[0]["generated_text"].strip()

    # 5) Return exactly the shape used in rag_pipeline.py
    return {
        "claim": claim,
        "triples": triples,
        "pubmed_abstracts": abstracts,
        "llm_output": raw,
    }

# verify_claim(model=base_model,
#              claim="Full mRNA vaccination dramatically increases the risk of symptomatic COVID-19 infection.",
#              kg_triples=[("COVID-19", "treats", "mRNA")], use_pubmed=True, use_kg=True, use_cot=True)

import json
import torch
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support
from collections import Counter
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

LABELS = ["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO"]

def normalize_token(tok_str):
    tok_str = tok_str.upper().strip()
    if tok_str.startswith("SUPPORT"):
        return "SUPPORTS"
    if tok_str.startswith("REFUTE"):
        return "REFUTES"
    return "NOT_ENOUGH_INFO"

!wget https://raw.githubusercontent.com/muskan-k/KGRAG-MedClaim/refs/heads/feature/sft-tanay/data/healthver/test.jsonl

# Load test data
with open("test.jsonl") as f:
    data = [json.loads(l) for l in f]

import re
def validate(data, model, tokenizer, message, use_pubmed, use_kg, use_cot, use_ift):
  preds, gts = [], []
  result = ""
  for ex in tqdm(data, desc=message):
      claim = ex["prompt"].split("\n")[0].split(":")[1].strip()
      # triples  = get_triples_by_entity_similarity(claim, limit=20)
      # result = verify_claim(model, tokenizer, claim, kg_triples=triples,
      #                       use_pubmed=use_pubmed,
      #                       use_kg=use_kg,
      #                       use_cot=use_cot,
      #                       use_ift=use_ift)["llm_output"]
      try:
        # verdict_match = re.search(r'"verdict":\s*"([^"]+)"', result)
        # if verdict_match:
        #     pred_text = verdict_match.group(1)
        #     pred = normalize_token(pred_text)
          pred = normalize_token("NOT_ENOUGH_INFO")
          gold = normalize_token(ex["completion"])

          preds.append(pred)
          gts.append(gold)

      except Exception as e:
        print("\nException:", e)
  acc = accuracy_score(gts, preds)
  prec = precision_score(gts, preds, labels=LABELS, average="macro")
  rec  = recall_score(gts, preds, labels=LABELS, average="macro")
  f1  = f1_score(gts, preds, labels=LABELS, average="macro")

  print(f"\n📊 Results:")
  print(f"Accuracy: {acc:.4f}")
  print(f"Macro F1: {f1:.4f}")
  print(f"Macro Precision: {prec:.4f}")
  print(f"Macro Recall: {rec:.4f}")
  print("Prediction Distribution:", Counter(preds))
  print("Ground Truth Distribution:", Counter(gts))

  # confusion matrix
  cm = confusion_matrix(gts, preds, labels=LABELS)
  sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, fmt="d", cmap="Blues")
  plt.xlabel("Predicted")
  plt.ylabel("Actual")
  plt.title("Confusion Matrix")
  plt.show()

  precisions, recalls, f1s, _ = precision_recall_fscore_support(gts, preds, labels=LABELS)
  plt.figure()
  plt.bar(LABELS, precisions)
  plt.title("Per-Class Precision")
  plt.show()

  plt.figure()
  plt.bar(LABELS, recalls)
  plt.title("Per-Class Recall")
  plt.show()

  plt.figure()
  plt.bar(LABELS, f1s)
  plt.title("Per-Class F1 Score")
  plt.show()

#### Base Model Validation

##### PubMed Only(uses BM25)

validate(data[:1001], base_model, base_tokenizer, "Evaluating PubMed Only with Base Model", use_pubmed=True, use_kg=False, use_cot=False)

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)
tokenizer.model_max_length = SEQ_LEN

model = PeftModel.from_pretrained(
    base_model,
    MODEL_DIR,
    local_files_only=True
)

model = model.merge_and_unload()

print("Merged model class:", base_model.__class__.__name__)

print("Merged model class:", model.__class__.__name__)

##### PubMed + KG with SFT Model without CoT & IFT

validate(data[:1000], base_model, base_tokenizer, "Evaluating Base Model - All Not Enough Info", use_pubmed=True, use_kg=False, use_cot=False, use_ift=False)

validate(data[:1000], base_model, base_tokenizer, "Evaluating PubMed with Base Model", use_pubmed=True, use_kg=True, use_cot=False, use_ift=False)

validate(data[:1000], model, tokenizer, "Evaluating PubMed+KG with SFT Model with Chain-of-Thought and without Instruction Prompt Tuning", use_pubmed=True, use_kg=True, use_cot=True, use_ift=False)

validate(data[:1000], model, tokenizer, "Evaluating PubMed+KG with SFT Model without Chain-of-Thought and with Instruction Prompt Tuning", use_pubmed=True, use_kg=True, use_cot=False, use_ift=True)

validate(data[:1000], model, tokenizer, "Evaluating PubMed+KG with SFT Model with Chain-of-Thought and Instruction Prompt Tuning", use_pubmed=True, use_kg=True, use_cot=True, use_ift=True)



